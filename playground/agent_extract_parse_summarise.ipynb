{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents - Decision based Document Summazization Technique\n",
    "## Objectove\n",
    "Build an agentic workflow to demonstrate how agents can be used to perform cost-effective document summarization of PDF content containing text and images utilizing multiple PDF summarization tools.\n",
    "## Implementation\n",
    "- Use AI Agent Framework (Autogen 0.4.*) to run multiple agents\n",
    "- Use AI Agent to make decisions based on input PDF\n",
    "- Summarize any given source PDF to MarkDown(.md) content\n",
    "- Basic free PDF Summarization tool is Microsoft's latest open source MarkItDown library\n",
    "- LLM PDF Summarization tool is Gemini 1.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogen-agentchat==0.4.5\n",
      "  Using cached autogen_agentchat-0.4.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting autogen-core==0.4.5 (from autogen-agentchat==0.4.5)\n",
      "  Using cached autogen_core-0.4.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in c:\\python311\\lib\\site-packages (from autogen-core==0.4.5->autogen-agentchat==0.4.5) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in c:\\python311\\lib\\site-packages (from autogen-core==0.4.5->autogen-agentchat==0.4.5) (1.29.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in c:\\python311\\lib\\site-packages (from autogen-core==0.4.5->autogen-agentchat==0.4.5) (11.1.0)\n",
      "Collecting protobuf~=5.29.3 (from autogen-core==0.4.5->autogen-agentchat==0.4.5)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in c:\\python311\\lib\\site-packages (from autogen-core==0.4.5->autogen-agentchat==0.4.5) (2.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\python311\\lib\\site-packages (from autogen-core==0.4.5->autogen-agentchat==0.4.5) (4.12.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\python311\\lib\\site-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat==0.4.5) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\python311\\lib\\site-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat==0.4.5) (8.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.5->autogen-agentchat==0.4.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.5->autogen-agentchat==0.4.5) (2.27.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\python311\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat==0.4.5) (1.17.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\python311\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.4.5->autogen-agentchat==0.4.5) (3.21.0)\n",
      "Using cached autogen_agentchat-0.4.5-py3-none-any.whl (64 kB)\n",
      "Using cached autogen_core-0.4.5-py3-none-any.whl (78 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Installing collected packages: protobuf, autogen-core, autogen-agentchat\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.6\n",
      "    Uninstalling protobuf-4.25.6:\n",
      "      Successfully uninstalled protobuf-4.25.6\n",
      "  Attempting uninstall: autogen-core\n",
      "    Found existing installation: autogen-core 0.4.2\n",
      "    Uninstalling autogen-core-0.4.2:\n",
      "      Successfully uninstalled autogen-core-0.4.2\n",
      "  Attempting uninstall: autogen-agentchat\n",
      "    Found existing installation: autogen-agentchat 0.4.2\n",
      "    Uninstalling autogen-agentchat-0.4.2:\n",
      "      Successfully uninstalled autogen-agentchat-0.4.2\n",
      "Successfully installed autogen-agentchat-0.4.5 autogen-core-0.4.5 protobuf-5.29.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ruban\\AppData\\Roaming\\Python\\Python311\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\n",
      "paddlepaddle 2.6.2 requires protobuf<=3.20.2,>=3.1.0; platform_system == \"Windows\", but you have protobuf 5.29.3 which is incompatible.\n",
      "streamlit 1.38.0 requires pillow<11,>=7.1.0, but you have pillow 11.1.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\python311\\lib\\site-packages (1.25.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install pip dependenices\n",
    "!pip install autogen-agentchat==0.4.5 --user\n",
    "!pip install PyMuPDF --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "pdf_path = \"original/image_and_text_sample.pdf\"\n",
    "output_dir = \"output/\"\n",
    "# Configure Google Cloud project\n",
    "PROJECT_ID = \"cryptic-skyline-411516\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents for process\n",
    "# pdf_path = \"original/image_and_text_sample.pdf\"           \n",
    "# pdf_path = \"original/text_sample.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import nest_asyncio\n",
    "from typing import AsyncGenerator, List, Sequence, Tuple\n",
    "import traceback\n",
    "import asyncio\n",
    "import fitz\n",
    "from pathlib import Path\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import BaseChatAgent\n",
    "from autogen_agentchat.base import Response\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage, StopMessage\n",
    "from autogen_agentchat.conditions import TextMentionTermination, HandoffTermination, StopMessageTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from markitdown import MarkItDown\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, Image\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_without_extension(file_path):\n",
    "    return os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "# define agents\n",
    "class UserProxyAgent(BaseChatAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name, \"A human user participating in the chat.\")\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> List[type[ChatMessage]]:\n",
    "        return [TextMessage]\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        user_input = await asyncio.get_event_loop().run_in_executor(None, input, \"Enter source PDF document Path:\")\n",
    "        print(\"input:\",user_input)\n",
    "        return Response(chat_message=TextMessage(content=user_input, source=self.name))\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        print(\"User Proxy Reset\")\n",
    "\n",
    "# Custom UserProxyAgent to interact with Gemini\n",
    "class GeminiAgent(BaseChatAgent):\n",
    "    def __init__(self, name: str, output_dir: str, mime_type : str = None):\n",
    "        super().__init__(name=name, description=\"An agent that converts images to text.\")\n",
    "        self._output_dir = output_dir\n",
    "        self._mime_type = mime_type\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "        \n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        # Call Gemini using the Google Cloud SDK\n",
    "        if not messages:\n",
    "            return Response(chat_message=StopMessage(content=\"No File Name received!\", source=self.name))\n",
    "        last_message = messages[-1].content.strip()\n",
    "        filePath = last_message\n",
    "        print(\"file:\",filePath)\n",
    "        response = await self.process(filePath=filePath, mime_type=self._mime_type)\n",
    "        # Return the generated response from Gemini\n",
    "        final_msg = TextMessage(content=f\"created:{filePath}\", source=self.name)\n",
    "        return Response(chat_message=final_msg)\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "    \n",
    "    async def process(self, filePath: str, mime_type: str = None) -> str:\n",
    "        try:\n",
    "            model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "            if(mime_type == \"application/pdf\"):\n",
    "                print(\"summarise pdf...\")            \n",
    "                encoded=self.encode_pdf(filePath)\n",
    "                #print(\"encoded\", base64.b64decode(encoded))\n",
    "                attachment_file = Part.from_data( mime_type=\"application/pdf\",\n",
    "                                        data=encoded)\n",
    "                prompt = \"Can you summarise content including images from the attached pdf in markdown?\"\n",
    "            else:\n",
    "                print(\"summarise image...\")  \n",
    "                attachment_file = Part.from_image(Image.load_from_file(filePath))\n",
    "                prompt = \"Describe this image?\"\n",
    "            # Query the model\n",
    "            print(\"total_tokens: \", model.count_tokens([attachment_file, prompt]))\n",
    "            response = model.generate_content([attachment_file, prompt])\n",
    "            print(\"usage_metadata: \", response.usage_metadata)\n",
    "            filename = os.path.join(self._output_dir, get_filename_without_extension(filePath))+\"_gemini.md\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(response.text)\n",
    "            return f\"created:{filename}\"\n",
    "        except Exception as e:\n",
    "            print(\"Exception : \", traceback.format_exc())\n",
    "            return f\"Error calling Vertex AI: {str(e)}\"\n",
    "        \n",
    "    def encode_pdf(self, pdf_path):\n",
    "        return Path(pdf_path).read_bytes()\n",
    "\n",
    "class MarkItDownAgent(BaseChatAgent):\n",
    "    def __init__(self, name, output_dir: str):\n",
    "        super().__init__(name, \"An agent to convert to Mark Down output.\")\n",
    "        self._output_dir = output_dir\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        if not messages:\n",
    "            return Response(chat_message=StopMessage(content=\"No File Name received!\", source=self.name))\n",
    "        last_message = messages[-1].content.strip()\n",
    "        filePath = last_message\n",
    "        return await self.process(filePath)\n",
    "\n",
    "    async def process(self, filePath: str) -> Response:\n",
    "        inner_messages: List[ChatMessage] = []\n",
    "        md = MarkItDown()\n",
    "        inner_messages.append(TextMessage(content=f'Will Summarise Free option using MarkDown...', source=self.name))\n",
    "        result = md.convert(filePath)\n",
    "        filename = os.path.join(self._output_dir, get_filename_without_extension(filePath))+\"_markitdown.md\"\n",
    "        print(\"content:\",result.text_content)\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(result.text_content)\n",
    "        #print(result.text_content)\n",
    "        final_msg = TextMessage(content=f\"created:{filename}\", source=self.name)\n",
    "        inner_messages.append(final_msg)\n",
    "        return Response(chat_message=final_msg)\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "class DocumentTriageJobAgent(BaseChatAgent):\n",
    "    def __init__(self, name: str):\n",
    "        super().__init__(name, \"An agent to decide on how which agent to use for processing document.\")\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        if not messages:\n",
    "            return Response(chat_message=StopMessage(content=\"No File Name received!\", source=self.name))\n",
    "        last_message = messages[-1].content.strip()\n",
    "        filePath = last_message\n",
    "        return await self.process(filePath)\n",
    "\n",
    "    async def process(self, filePath: str) -> Response:\n",
    "        inner_messages: List[ChatMessage] = []\n",
    "        if not(os.path.isfile(filePath)):\n",
    "            return Response(chat_message=StopMessage(content=\"File path provided isn't a valid file!\", source=self.name), inner_messages=inner_messages)\n",
    "        classifier_result = self.classifier(filePath)\n",
    "        inner_messages.append(TextMessage(content=f'pdf classifier_result: {classifier_result}', source=self.name))\n",
    "        print(\"document type:\",classifier_result)\n",
    "        if(classifier_result==\"TEXT\"):\n",
    "            agent = MarkItDownAgent(name=\"markdown_agent\", output_dir=output_dir)\n",
    "        elif (classifier_result==\"IMAGE\"):\n",
    "            agent = GeminiAgent(name=\"llm_agent\", output_dir=output_dir, mime_type=\"application/pdf\")\n",
    "        await Console(\n",
    "                agent.on_messages_stream(\n",
    "                    [TextMessage(content=filePath, source=\"user\")], CancellationToken()\n",
    "                )\n",
    "            )\n",
    "        #print(\"final:\",inner_messages)\n",
    "        final_msg = TextMessage(content=\"Done!\", source=self.name)\n",
    "        return Response(chat_message=final_msg, inner_messages=inner_messages)\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "    def classifier(self, pdf_file):\n",
    "        with open(pdf_file,\"rb\") as f:\n",
    "            pdf = fitz.open(f)\n",
    "            res = []\n",
    "            for page in pdf:\n",
    "                img_refs = page.get_image_info(xrefs=True)\n",
    "                if img_refs != []:\n",
    "                    #print(\"Page\", page.number, \"images:\", [i[\"xref\"] for i in img_refs])\n",
    "                    res.append([i[\"xref\"] for i in img_refs])\n",
    "            if  len(res) == 0:\n",
    "                return(\"TEXT\")\n",
    "            else:\n",
    "                return(\"IMAGE\")\n",
    "# debug run\n",
    "# Main function to run the agent\n",
    "# async def debug_agent(input_path: str):\n",
    "#     agent = DocumentTriageJobAgent(name=\"document_triage_agent\")\n",
    "#     # User's input message\n",
    "#     user_message = TextMessage(content=input_path, source=\"user\")\n",
    "#     # Send the message and get the response from Gemini\n",
    "#     response = await agent.on_messages(messages=[user_message],\n",
    "#         cancellation_token=CancellationToken(),\n",
    "#     )\n",
    "    \n",
    "#     print(f\"response: {response.chat_message}\")\n",
    "\n",
    "# #Run the async function\n",
    "# await debug_agent(\"original/image_and_text_sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "---------- markdown_agent ----------\n",
      "No File Name received!\n"
     ]
    }
   ],
   "source": [
    "# initiate agents\n",
    "async def main():\n",
    "    # Initialize agents\n",
    "    user_proxy_agent = UserProxyAgent(name=\"user_proxy_agent\")\n",
    "    document_triage_agent = DocumentTriageJobAgent(name=\"document_triage_agent\")\n",
    "    markitdown_agent = MarkItDownAgent(name=\"markdown_agent\", output_dir=output_dir)\n",
    "    llm_agent = GeminiAgent(name=\"llm_agent\", output_dir=output_dir, mime_type=\"application/pdf\")\n",
    "    # Define termination conditions\n",
    "    termination = TextMentionTermination(\"Done!\") | HandoffTermination(user_proxy_agent) | StopMessageTermination()\n",
    "    # Create a chat group\n",
    "    group_chat = RoundRobinGroupChat(\n",
    "        participants=[user_proxy_agent, document_triage_agent],\n",
    "        termination_condition=termination\n",
    "    )\n",
    "\n",
    "    group_chat = SelectorGroupChat(\n",
    "        [markitdown_agent, llm_agent],\n",
    "        model_client=model_client,\n",
    "        selector_func=selector_func,\n",
    "        termination_condition=termination,\n",
    "        max_turns=3\n",
    "    )\n",
    "\n",
    "    # Programmatically provide the initial message\n",
    "    initial_message = TextMessage(content=\"Enter Source file Name:\", source=\"user\")\n",
    "    # Start the chat\n",
    "    stream = group_chat.run_stream(cancellation_token=CancellationToken())\n",
    "    # Await the stream to process messages\n",
    "    await Console(stream)\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
