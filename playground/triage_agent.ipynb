{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
    "\n",
    "# Define the TriageAgent with custom logic\n",
    "class TriageAgent(AssistantAgent):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(\n",
    "            name,\n",
    "            llm_config=False,  # Disable LLM\n",
    "            system_message=\"You are the Triage Agent. Analyze the user's message and decide which agent to route to.\"\n",
    "        )\n",
    "\n",
    "    def decide_next_agent(self, message):\n",
    "        if \"support\" in message.lower():\n",
    "            return \"SupportAgent\"\n",
    "        elif \"technical\" in message.lower():\n",
    "            return \"TechnicalAgent\"\n",
    "        else:\n",
    "            return \"SupportAgent\"  # Default to SupportAgent\n",
    "\n",
    "    def generate_reply(self, messages=None, sender=None, **kwargs):\n",
    "        # Extract the last message from the user\n",
    "        print(\"triage message:\",messages)\n",
    "        if messages:\n",
    "            last_message = messages[-1][\"content\"]\n",
    "            next_agent = self.decide_next_agent(last_message)\n",
    "            return f\"Routing to {next_agent}.\"\n",
    "        else:\n",
    "            return \"No messages to process. Please start the conversation.\"\n",
    "\n",
    "# Define the SupportAgent with custom logic\n",
    "class SupportAgent(AssistantAgent):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(\n",
    "            name,\n",
    "            llm_config=False,  # Disable LLM\n",
    "            system_message=\"You are the Support Agent. Provide helpful and friendly support for general inquiries.\"\n",
    "        )\n",
    "\n",
    "    def generate_reply(self, messages=None, sender=None, **kwargs):\n",
    "        return \"Thank you for contacting support. How can we assist you today?\"\n",
    "\n",
    "# Define the TechnicalAgent with custom logic\n",
    "class TechnicalAgent(AssistantAgent):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(\n",
    "            name,\n",
    "            llm_config=False,  # Disable LLM\n",
    "            system_message=\"You are the Technical Agent. Provide detailed and technical assistance for complex issues.\"\n",
    "        )\n",
    "\n",
    "    def generate_reply(self, messages=None, sender=None, **kwargs):\n",
    "        return \"Thank you for contacting technical support. Please describe the issue you're facing.\"\n",
    "\n",
    "# Define the UserProxyAgent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"UserProxy\",\n",
    "    human_input_mode=\"NEVER\",  # Set to \"NEVER\" for fully automated chat\n",
    "    code_execution_config=False\n",
    ")\n",
    "\n",
    "# Create instances of the agents\n",
    "triage_agent = TriageAgent(\"TriageAgent\")\n",
    "support_agent = SupportAgent(\"SupportAgent\")\n",
    "technical_agent = TechnicalAgent(\"TechnicalAgent\")\n",
    "\n",
    "# Define the group chat and manager\n",
    "groupchat = GroupChat(\n",
    "    agents=[user_proxy, triage_agent,technical_agent, support_agent ],\n",
    "    messages=[],\n",
    "    max_round=10,\n",
    "    speaker_selection_method=\"round_robin\"  # Ensures the TriageAgent gets a turn\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(groupchat=groupchat)\n",
    "\n",
    "# Start the conversation\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"I need technical help with my account login. It's not working.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import AsyncGenerator, List, Sequence, Tuple\n",
    "from autogen_agentchat.base import Response\n",
    "from autogen_agentchat.agents import AssistantAgent, BaseChatAgent\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    def check_calculation(x: int, y: int, answer: int) -> str:\n",
    "        if x + y == answer:\n",
    "            return \"Correct!\"\n",
    "        else:\n",
    "            return \"Incorrect!\"\n",
    "    \n",
    "    class CalcAgent(BaseChatAgent):\n",
    "        def __init__(self, name: str):\n",
    "            super().__init__(name,name)\n",
    "\n",
    "        @property\n",
    "        def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n",
    "            return (TextMessage,)\n",
    "\n",
    "        async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "            last_message = messages[-1].content.strip()\n",
    "            count = int(last_message)\n",
    "            return await self.process(count)\n",
    "\n",
    "        async def process(self, count: int) -> Response:\n",
    "            inner_messages: List[ChatMessage] = []\n",
    "            final_msg = TextMessage(content=\"Done!\", source=self.name)\n",
    "            inner_messages.append(final_msg)\n",
    "            print(\"final:\",final_msg.content)\n",
    "            return Response(chat_message=final_msg, inner_messages=inner_messages)\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "    agent1 = AssistantAgent(\n",
    "        \"Agent1\",\n",
    "        model_client,\n",
    "        description=\"For calculation\",\n",
    "        system_message=\"Calculate the sum of two numbers\",\n",
    "    )\n",
    "    agent2 = AssistantAgent(\n",
    "        \"Agent2\",\n",
    "        model_client,\n",
    "        tools=[check_calculation],\n",
    "        description=\"For checking calculation\",\n",
    "        system_message=\"Check the answer and respond with 'Correct!' or 'Incorrect!'\",\n",
    "    )\n",
    "\n",
    "    def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:\n",
    "        if len(messages) == 1 or messages[-1].content == \"Incorrect!\":\n",
    "            return \"Agent1\"\n",
    "        if messages[-1].source == \"Agent1\":\n",
    "            return \"Agent2\"\n",
    "        return None\n",
    "\n",
    "    termination = TextMentionTermination(\"Correct!\")\n",
    "    team = SelectorGroupChat(\n",
    "        [agent1, agent2],\n",
    "        model_client=model_client,\n",
    "        selector_func=selector_func,\n",
    "        termination_condition=termination,\n",
    "    )\n",
    "\n",
    "    await Console(team.run_stream(task=\"What is 1 + 1?\"))\n",
    "\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to User_proxy):\n",
      "\n",
      "Starting the conversation. Enter your message:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser_proxy\u001b[0m (to User_proxy):\n",
      "\n",
      "keyword1\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ConversableAgent.get_human_input() missing 1 required positional argument: 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Code\\ml\\playground\\triage_agent.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m user_proxy\u001b[39m.\u001b[39minitiate_chat(user_proxy, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting the conversation. Enter your message:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     user_input \u001b[39m=\u001b[39m user_proxy\u001b[39m.\u001b[39;49mget_human_input()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m user_input\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ConversableAgent.get_human_input() missing 1 required positional argument: 'prompt'"
     ]
    }
   ],
   "source": [
    "from autogen import UserProxyAgent, AssistantAgent, GroupChat\n",
    "\n",
    "# Define agents (LLM disabled)\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config=False\n",
    ")\n",
    "agent1 = AssistantAgent(\n",
    "    name=\"Agent1\",\n",
    "    llm_config=False,  # Empty config list to disable LLM\n",
    ")\n",
    "agent2 = AssistantAgent(\n",
    "    name=\"Agent2\",\n",
    "    llm_config={\"config_list\": []},  # Empty config list to disable LLM\n",
    ")\n",
    "\n",
    "# Define a simple routing function\n",
    "def route_message(message):\n",
    "    if \"keyword1\" in message:\n",
    "        return agent1\n",
    "    elif \"keyword2\" in message:\n",
    "        return agent2\n",
    "    else:\n",
    "        return None  # Or a default agent\n",
    "\n",
    "# Start chat, manually routing messages\n",
    "user_proxy.initiate_chat(agent1, message=\"Starting the conversation.\") # Initial message\n",
    "\n",
    "while True:\n",
    "    user_input = user_proxy.get_human_input(\"Enter your message (or 'exit'): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    recipient = route_message(user_input)\n",
    "\n",
    "    if recipient:\n",
    "        user_proxy.send(user_input, recipient)\n",
    "        response = recipient.receive()  # Manually get the response.\n",
    "        if response:\n",
    "            print(f\"{recipient.name}: {response['content']}\")\n",
    "    else:\n",
    "        print(\"No suitable agent found for this message.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Code\\ml\\playground\\triage_agent.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mprint\u001b[39m(message)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m nest_asyncio\u001b[39m.\u001b[39mapply()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(main())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[0;32m     31\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\u001b[39m.\u001b[39mwith_traceback(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "\u001b[1;32mc:\\Code\\ml\\playground\\triage_agent.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model_client \u001b[39m=\u001b[39m OpenAIChatCompletionClient(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4o\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     agent1 \u001b[39m=\u001b[39m AssistantAgent(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAlice\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         model_client\u001b[39m=\u001b[39mmodel_client,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         handoffs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mBob\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         system_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are Alice and you only answer questions about yourself.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     agent2 \u001b[39m=\u001b[39m AssistantAgent(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBob\u001b[39m\u001b[39m\"\u001b[39m, model_client\u001b[39m=\u001b[39mmodel_client, system_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are Bob and your birthday is on 1st January.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Code/ml/playground/triage_agent.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:1066\u001b[0m, in \u001b[0;36mOpenAIChatCompletionClient.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1063\u001b[0m     model_info \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel_info\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1064\u001b[0m     \u001b[39mdel\u001b[39;00m copied_args[\u001b[39m\"\u001b[39m\u001b[39mmodel_info\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m-> 1066\u001b[0m client \u001b[39m=\u001b[39m _openai_client_from_config(copied_args)\n\u001b[0;32m   1067\u001b[0m create_args \u001b[39m=\u001b[39m _create_args_from_config(copied_args)\n\u001b[0;32m   1068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw_config: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m copied_args\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:109\u001b[0m, in \u001b[0;36m_openai_client_from_config\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_openai_client_from_config\u001b[39m(config: Mapping[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AsyncOpenAI:\n\u001b[0;32m    107\u001b[0m     \u001b[39m# Shave down the config to just the OpenAI kwargs\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     openai_config \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m openai_init_kwargs}\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m AsyncOpenAI(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopenai_config)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_client.py:337\u001b[0m, in \u001b[0;36mAsyncOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    335\u001b[0m     api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    336\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m     )\n\u001b[0;32m    340\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m api_key\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m organization \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import Swarm\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "\n",
    "\n",
    "async def main() -> None:\n",
    "    #model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
    "\n",
    "    agent1 = AssistantAgent(\n",
    "        \"Alice\",\n",
    "        llm,\n",
    "        handoffs=[\"Bob\"],\n",
    "        system_message=\"You are Alice and you only answer questions about yourself.\",\n",
    "    )\n",
    "    agent2 = AssistantAgent(\n",
    "        \"Bob\", model_client=model_client, system_message=\"You are Bob and your birthday is on 1st January.\"\n",
    "    )\n",
    "\n",
    "    termination = MaxMessageTermination(3)\n",
    "    team = Swarm([agent1, agent2], termination_condition=termination)\n",
    "\n",
    "    stream = team.run_stream(task=\"What is bob's birthday?\")\n",
    "    async for message in stream:\n",
    "        print(message)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Your name is s\n",
      "2.Your name is n\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.agents import UserProxyAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "\n",
    "async def simple_user_agent():\n",
    "    agent1 = UserProxyAgent(\"user_proxy\")\n",
    "    agent2 = UserProxyAgent(\"user_proxy\")\n",
    "    response = await asyncio.create_task(\n",
    "        agent1.on_messages(\n",
    "            [TextMessage(content=\"What is your name? \", source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "    )\n",
    "    print(f\"1.Your name is {response.chat_message.content}\")\n",
    "    response = await asyncio.create_task(\n",
    "        agent2.on_messages(\n",
    "            [TextMessage(content=\"What is your name? \", source=\"user\")],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "    )\n",
    "    print(f\"2.Your name is {response.chat_message.content}\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "asyncio.run(simple_user_agent())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
