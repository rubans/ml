{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfQYl84Cu_xL"
      },
      "source": [
        "## Wikilingua Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxAXgV2FvBPz"
      },
      "source": [
        "The dataset includes article and summary pairs from WikiHow. It consists of  article-summary pairs in multiple languages. Refer to the following [github repository](https://github.com/esdurmus/Wikilingua) for more details.\n",
        "\n",
        "For this notebook, we have picked `english` language dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTHBNpb-BBdc"
      },
      "source": [
        "### Dataset Citation\n",
        "\n",
        "```\n",
        "@inproceedings{ladhak-wiki-2020,\n",
        "    title={WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization},\n",
        "    author={Faisal Ladhak, Esin Durmus, Claire Cardie and Kathleen McKeown},\n",
        "    booktitle={Findings of EMNLP, 2020},\n",
        "    year={2020}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use Cases\n",
        "* Explainability\n",
        "* Output in Particular Format such as JSON or Template (Structured Documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llEFILYz2aye"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo2rh4cC2e1r"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l_ok3vdw2cyf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform rouge_score plotly jsonlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8CI-TcqD06L"
      },
      "source": [
        "## Step1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rerpHL_eEG8D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# For extracting vertex experiment details.\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.metadata import context\n",
        "from google.cloud.aiplatform.metadata import utils as metadata_utils\n",
        "import jsonlines\n",
        "\n",
        "# For data handling.\n",
        "import pandas as pd\n",
        "\n",
        "# For visualization.\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# For evaluation metric computation.\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For fine tuning Gemini model.\n",
        "import vertexai\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from vertexai.preview.tuning import sft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBY9nK3qEJLk"
      },
      "source": [
        "## Step2: Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VpzmI1K61Tn2"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"cryptic-skyline-411516\"\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUEloBlsCPFr"
      },
      "source": [
        "## Step3: Create Dataset in correct format\n",
        "\n",
        "The dataset used to tune a foundation model needs to include examples that align with the task that you want the model to perform. Structure your training dataset in a text-to-text format. Each record, or row, in the dataset contains the input text (also referred to as the prompt) which is paired with its expected output from the model. Supervised tuning uses the dataset to teach the model to mimic a behavior, or task, you need by giving it hundreds of examples that illustrate that behavior.\n",
        "\n",
        "Your dataset size depends on the task, and follows the recommendation mentioned in the `Overview` section. The more examples you provide in your dataset, the better the results.\n",
        "\n",
        "### Dataset format\n",
        "\n",
        "Training data should be structured within a JSONL file located at a Google Cloud Storage (GCS) URI. Each line (or row) of the JSONL file must adhere to a specific schema: It should contain a `contents` array, with objects inside defining a `role` (either \"user\" for user input or \"model\" for model output) and `parts`, containing the input data. For example, a valid data row would look like this:\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "   \"contents\":[\n",
        "      {\n",
        "         \"role\":\"user\",  # This indicate input content\n",
        "         \"parts\":[\n",
        "            {\n",
        "               \"text\":\"How are you?\"\n",
        "            }\n",
        "         ]\n",
        "      },\n",
        "      {\n",
        "         \"role\":\"model\", # This indicate target content\n",
        "         \"parts\":[ # text only\n",
        "            {\n",
        "               \"text\":\"I am good, thank you!\"\n",
        "            }\n",
        "         ]\n",
        "      }\n",
        "      #  ... repeat \"user\", \"model\" for multi turns.\n",
        "   ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Refer to the public [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare#about-datasets) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TglfI3tQr2oZ"
      },
      "source": [
        "To run a tuning job, you need to upload one or more datasets to a Cloud Storage bucket. You can either create a new Cloud Storage bucket or use an existing one to store dataset files. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsNdYgnaITuz"
      },
      "source": [
        "### Step3 [a]: Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WveeKANmITK5"
      },
      "outputs": [],
      "source": [
        "# Provide a bucket name\n",
        "BUCKET_NAME = \"training_data\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}_{PROJECT_ID}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBSGTEiyJfSR"
      },
      "source": [
        "Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GQSJ3LJkJhLm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating gs://training_data_cryptic-skyline-411516/...\n",
            "ServiceException: 409 A Cloud Storage bucket named 'training_data_cryptic-skyline-411516' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YGurtXHJy_y"
      },
      "source": [
        "### Step3 [b]: Upload tuning data to Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip8rErN2r3ah"
      },
      "source": [
        "- Data used in this notebook is present in the public Google Cloud Storage(GCS) bucket.\n",
        "- It's in Gemini 1.0 finetuning dataset format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aV_GZg_LaNmV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/\n",
            "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_test_samples.csv\n",
            "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_train_samples.jsonl\n",
            "gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_val_samples.jsonl\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b-EQ4FcExIfp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_test_samples.csv...\n",
            "/ [0 files][    0.0 B/235.8 KiB]                                                \n",
            "/ [1 files][235.8 KiB/235.8 KiB]                                                \n",
            "Copying gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_train_samples.jsonl...\n",
            "/ [1 files][235.8 KiB/  1.4 MiB]                                                \n",
            "-\n",
            "- [2 files][  1.4 MiB/  1.4 MiB]                                                \n",
            "Copying gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/sft_val_samples.jsonl...\n",
            "- [2 files][  1.4 MiB/  1.6 MiB]                                                \n",
            "\\\n",
            "\\ [3 files][  1.6 MiB/  1.6 MiB]                                                \n",
            "\n",
            "Operation completed over 3 objects/1.6 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://github-repo/generative-ai/gemini/tuning/summarization/wikilingua/* ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wmBkAUoyzdJ"
      },
      "source": [
        "#### Convert Gemini 1.0 tuning dataset to Gemini 1.5 tuning dataset format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7bhtxKbha_wS"
      },
      "outputs": [],
      "source": [
        "def save_jsonlines(file, instances):\n",
        "    \"\"\"\n",
        "    Saves a list of json instances to a jsonlines file.\n",
        "    \"\"\"\n",
        "    with jsonlines.open(file, mode=\"w\") as writer:\n",
        "        writer.write_all(instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-3xbpRnSyxH9"
      },
      "outputs": [],
      "source": [
        "def create_tuning_samples(file_path):\n",
        "    \"\"\"\n",
        "    Creates tuning samples from a file.\n",
        "    \"\"\"\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        instances = []\n",
        "        for obj in reader:\n",
        "            instance = []\n",
        "            for content in obj[\"messages\"]:\n",
        "                instance.append(\n",
        "                    {\"role\": content[\"role\"], \"parts\": [{\"text\": content[\"content\"]}]}\n",
        "                )\n",
        "            instances.append({\"contents\": instance})\n",
        "    return instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "orHUpTagyw-z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_file = \"sft_train_samples.jsonl\"\n",
        "train_instances = create_tuning_samples(train_file)\n",
        "len(train_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U3m05gAM1mlp"
      },
      "outputs": [],
      "source": [
        "# save the training instances to jsonl file\n",
        "save_jsonlines(train_file, train_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xZICW6uU1T3Z"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_file = \"sft_val_samples.jsonl\"\n",
        "val_instances = create_tuning_samples(val_file)\n",
        "len(val_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n1s7adZH1CCe"
      },
      "outputs": [],
      "source": [
        "# save the validation instances to jsonl file\n",
        "save_jsonlines(val_file, val_instances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AVL2gfP-J5SL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying file://sft_train_samples.jsonl [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/  1.2 MiB]                                                \n",
            "-\n",
            "- [0 files][960.0 KiB/  1.2 MiB]                                                \n",
            "- [1 files][  1.2 MiB/  1.2 MiB]                                                \n",
            "\\\n",
            "\n",
            "Operation completed over 1 objects/1.2 MiB.                                      \n",
            "Copying file://sft_val_samples.jsonl [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/231.9 KiB]                                                \n",
            "/ [1 files][231.9 KiB/231.9 KiB]                                                \n",
            "-\n",
            "\n",
            "Operation completed over 1 objects/231.9 KiB.                                    \n"
          ]
        }
      ],
      "source": [
        "# Copy the tuning and evaluation data to your bucket.\n",
        "!gsutil cp {train_file} {BUCKET_URI}/train/\n",
        "!gsutil cp {val_file} {BUCKET_URI}/val/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV5X6_DsIXPm"
      },
      "source": [
        "### Step3 [c]: Test dataset\n",
        "\n",
        "- It contains document text(`input_text`) and corresponding reference summary(`output_text`), which will be compared with the model generated summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wtxPI3GPIckU"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>output_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hold your arm out flat in front of you with yo...</td>\n",
              "      <td>Squeeze a line of lotion onto the tops of both...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>As you continue playing, surviving becomes pai...</td>\n",
              "      <td>Make a Crock Pot for better food. Create an Al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go to https://www.4kdownload.com/products/prod...</td>\n",
              "      <td>Download the 4K Video Downloader setup file. I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You should know that vaginoplasty can treat a ...</td>\n",
              "      <td>Consider the health of your bladder. Find a so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you want to gather data on the frequency of...</td>\n",
              "      <td>Gather data to be graphed. Choose your range b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          input_text  \\\n",
              "0  Hold your arm out flat in front of you with yo...   \n",
              "1  As you continue playing, surviving becomes pai...   \n",
              "2  Go to https://www.4kdownload.com/products/prod...   \n",
              "3  You should know that vaginoplasty can treat a ...   \n",
              "4  If you want to gather data on the frequency of...   \n",
              "\n",
              "                                         output_text  \n",
              "0  Squeeze a line of lotion onto the tops of both...  \n",
              "1  Make a Crock Pot for better food. Create an Al...  \n",
              "2  Download the 4K Video Downloader setup file. I...  \n",
              "3  Consider the health of your bladder. Find a so...  \n",
              "4  Gather data to be graphed. Choose your range b...  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the test dataset using pandas as it's in the csv format.\n",
        "testing_data_path = \"sft_test_samples.csv\"\n",
        "test_data = pd.read_csv(testing_data_path)\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bRBtYfN_PPaP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hold your arm out flat in front of you with your elbow bent. The top of your forearm should form a level surface. Apply a line of lotion from the back of your hand up your arm almost to the crease of your elbow. Squeeze lotion onto both forearms.  Do not rub the lotion into your arms, rather let it sit on your arm in the line you squeezed. You can use as much or as little lotion as you feel is necessary to cover your back completely. Bend your elbows and reach both of your arms behind you, placing the lotion covered forearms against your back. Depending on how flexible you are, this may hurt a little. It might be easier to place one arm behind your back at a time. If you have shoulder pain or are not very flexible, this method may not work well for you. Rub your forearms and the backs of your hands up and down your back like windshield wipers covering as much of your back as you can. You can use your left arm first to cover your left side and then place your right arm behind and use it to cover the right side of your back. Repeat this process as necessary if you don’t feel like you got enough lotion on your back.\\n\\nProvide a summary of the article in two or three sentences:\\n\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.loc[0, \"input_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tTt7qjSeSHRW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    100.000000\n",
              "mean     186.230000\n",
              "std       92.788655\n",
              "min       28.000000\n",
              "25%      127.250000\n",
              "50%      171.000000\n",
              "75%      227.000000\n",
              "max      577.000000\n",
              "Name: output_text, dtype: float64"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Article summary stats\n",
        "stats = test_data[\"output_text\"].apply(len).describe()\n",
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WKptd49cdjSi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total `100.0` test records\n",
            "Average length is `186.23` and max is `577.0` characters\n",
            "\n",
            "Considering 1 token = 4 chars\n",
            "\n",
            "Set max_token_length = stats['max']/4 = 144.25 ~ 145 characters\n",
            "\n",
            "Let's keep output tokens upto `145`\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total `{stats['count']}` test records\")\n",
        "print(f\"Average length is `{stats['mean']}` and max is `{stats['max']}` characters\")\n",
        "print(\"\\nConsidering 1 token = 4 chars\")\n",
        "\n",
        "# Get ceil value of the tokens required.\n",
        "tokens = (stats[\"max\"] / 4).__ceil__()\n",
        "print(\n",
        "    f\"\\nSet max_token_length = stats['max']/4 = {stats['max']/4} ~ {tokens} characters\"\n",
        ")\n",
        "print(f\"\\nLet's keep output tokens upto `{tokens}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "idM1p_UNvA7w"
      },
      "outputs": [],
      "source": [
        "# Maximum number of tokens that can be generated in the response by the LLM.\n",
        "# Experiment with this number to get optimal output.\n",
        "max_output_tokens = tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhjmRffOOPAS"
      },
      "source": [
        "## Step4: Load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhhD1VWDsLat"
      },
      "source": [
        "The following Gemini text models support supervised tuning:\n",
        "\n",
        "* `gemini-1.5-pro-002`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jL-zRl5_OVZW"
      },
      "outputs": [],
      "source": [
        "base_model = \"gemini-1.5-flash-002\"\n",
        "generation_model = GenerativeModel(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieJe8yGlOtFD"
      },
      "source": [
        "## Step5: Test the Gemini model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8DFUzRnHMi8"
      },
      "source": [
        "### Generation config\n",
        "\n",
        "- Each call that you send to a model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values\n",
        "- <strong>Experiment</strong> with different parameter values to get the best values for the task\n",
        "\n",
        "Refer to the following [link](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) for understanding different parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbaeT8AcniS"
      },
      "source": [
        "**Prompt** is a natural language request submitted to a language model to receive a response back\n",
        "\n",
        "Some best practices include\n",
        "  - Clearly communicate what content or information is most important\n",
        "  - Structure the prompt:\n",
        "    - Defining the role if using one. For example, You are an experienced UX designer at a top tech company\n",
        "    - Include context and input data\n",
        "    - Provide the instructions to the model\n",
        "    - Add example(s) if you are using them\n",
        "\n",
        "Refer to the following [link](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) for prompt design strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUcvQr0rAWA"
      },
      "source": [
        "Wikilingua data contains the following task prompt at the end of the article, `Provide a summary of the article in two or three sentences:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iu6OuIhFOv4C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This describes a method for applying lotion to one's back using one's forearms as applicators.  Lotion is applied to the forearms, then the forearms are used to rub the lotion onto the back in a windshield wiper motion.  This technique may not be suitable for everyone, particularly those with limited flexibility or shoulder pain.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_doc = test_data.loc[0, \"input_text\"]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{test_doc}\n",
        "\"\"\"\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    max_output_tokens=max_output_tokens,\n",
        ")\n",
        "\n",
        "response = generation_model.generate_content(\n",
        "    contents=prompt, generation_config=generation_config\n",
        ").text\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8YvlMfmIQqK8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Squeeze a line of lotion onto the tops of both forearms and the backs of your hands. Place your arms behind your back. Move your arms in a windshield wiper motion.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ground truth\n",
        "test_data.loc[0, \"output_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGPUKZlcP69-"
      },
      "source": [
        "## Step6: Evaluation before model tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yayTQdd9oE5"
      },
      "source": [
        "- Evaluate the Gemini model on the test dataset before tuning it on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "610J64SpQ5TE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the pandas dataframe to records (list of dictionaries).\n",
        "corpus = test_data.to_dict(orient=\"records\")\n",
        "# Check number of records.\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkKldH90MY4v"
      },
      "source": [
        "### Evaluation metric\n",
        "\n",
        "The type of metrics used for evaluation depends on the task that you are evaluating. The following table shows the supported tasks and the metrics used to evaluate each task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6oLtUEWMHVu"
      },
      "source": [
        "| Task             | Metric(s)                     |\n",
        "|-----------------|---------------------------------|\n",
        "| Classification   | Micro-F1, Macro-F1, Per class F1 |\n",
        "| Summarization    | ROUGE-L                         |\n",
        "| Question Answering | Exact Match                     |\n",
        "| Text Generation  | BLEU, ROUGE-L                   |\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "Refer to this [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluate-models) for metric based evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKNk3zG4CNSS"
      },
      "source": [
        "- **Recall-Oriented Understudy for Gisting Evaluation (ROUGE)**: A metric used to evaluate the quality of automatic summaries of text. It works by comparing a generated summary to a set of reference summaries created by humans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XP9VOaTd3z8"
      },
      "source": [
        "Now you can take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:\n",
        "\n",
        "- `rouge-1`, which measures unigram overlap\n",
        "- `rouge-2`, which measures bigram overlap\n",
        "- `rouge-l`, which measures the longest common subsequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2CrcvvzFfBL"
      },
      "source": [
        "#### *Recall vs. Precision*\n",
        "\n",
        "**Recall**, meaning it prioritizes how much of the information in the reference summaries is captured in the generated summary.\n",
        "\n",
        "**Precision**, which measures how much of the generated summary is relevant to the original text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU6K4YdaGGyp"
      },
      "source": [
        "<strong>Alternate Evaluation method</strong>: Check out the [AutoSxS](https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval) evaluation for automatic evaluation of the task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "p3YZAOZcQWtW"
      },
      "outputs": [],
      "source": [
        "# Create rouge_scorer object for evaluation\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "N8Y06N_b_EP5"
      },
      "outputs": [],
      "source": [
        "def run_evaluation(model: GenerativeModel, corpus: list[dict]) -> pd.DataFrame:\n",
        "    \"\"\"Runs evaluation for the given model and data.\n",
        "\n",
        "    Args:\n",
        "      model: The generation model.\n",
        "      corpus: The test data.\n",
        "\n",
        "    Returns:\n",
        "      A pandas DataFrame containing the evaluation results.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    for item in tqdm(corpus):\n",
        "        document = item.get(\"input_text\")\n",
        "        summary = item.get(\"output_text\")\n",
        "\n",
        "        # Catch any exception that occur during model evaluation.\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                document,\n",
        "                generation_config=generation_config,\n",
        "                safety_settings={\n",
        "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                },\n",
        "            )\n",
        "            print(response)\n",
        "\n",
        "            # Check if response is generated by the model, if response is empty then continue to next item.\n",
        "            if not (\n",
        "                response\n",
        "                and response.candidates\n",
        "                and response.candidates[0].content.parts\n",
        "            ):\n",
        "                print(\n",
        "                    f\"\\nModel has blocked the response for the document.\\n Response: {response}\\n Document: {document}\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            # Calculates the ROUGE score for a given reference and generated summary.\n",
        "            scores = scorer.score(target=summary, prediction=response.text)\n",
        "\n",
        "            # Append the results to the records list\n",
        "            records.append(\n",
        "                {\n",
        "                    \"document\": document,\n",
        "                    \"summary\": summary,\n",
        "                    \"generated_summary\": response.text,\n",
        "                    \"scores\": scores,\n",
        "                    \"rouge1_precision\": scores.get(\"rouge1\").precision,\n",
        "                    \"rouge1_recall\": scores.get(\"rouge1\").recall,\n",
        "                    \"rouge1_fmeasure\": scores.get(\"rouge1\").fmeasure,\n",
        "                    \"rouge2_precision\": scores.get(\"rouge2\").precision,\n",
        "                    \"rouge2_recall\": scores.get(\"rouge2\").recall,\n",
        "                    \"rouge2_fmeasure\": scores.get(\"rouge2\").fmeasure,\n",
        "                    \"rougeL_precision\": scores.get(\"rougeL\").precision,\n",
        "                    \"rougeL_recall\": scores.get(\"rougeL\").recall,\n",
        "                    \"rougeL_fmeasure\": scores.get(\"rougeL\").fmeasure,\n",
        "                }\n",
        "            )\n",
        "        except AttributeError as attr_err:\n",
        "            print(\"Attribute Error:\", attr_err)\n",
        "            continue\n",
        "        except Exception as err:\n",
        "            print(\"Error:\", err)\n",
        "            continue\n",
        "    return pd.DataFrame(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4afTSo5cpM73"
      },
      "outputs": [],
      "source": [
        "# Batch of test data.\n",
        "corpus_batch = corpus[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM10zigp7kTZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ It will take ~5 mins for the evaluation run on the provided batch. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \"This describes a method for applying lotion to one\\'s back using one\\'s forearms as makeshift applicators.  Lotion is applied to the forearms, then the forearms are used to rub the lotion onto the back in a windshield wiper motion.  This technique may not be suitable for everyone, particularly those with limited flexibility or shoulder pain.\\n\"\n",
            "    }\n",
            "  }\n",
            "  avg_logprobs: -0.028314865297741361\n",
            "  finish_reason: STOP\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_HATE_SPEECH\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.284576565\n",
            "    severity: HARM_SEVERITY_LOW\n",
            "    severity_score: 0.288568705\n",
            "  }\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.201813355\n",
            "    severity: HARM_SEVERITY_MEDIUM\n",
            "    severity_score: 0.444565088\n",
            "  }\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_HARASSMENT\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.321673483\n",
            "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "    severity_score: 0.134776011\n",
            "  }\n",
            "  safety_ratings {\n",
            "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
            "    probability: NEGLIGIBLE\n",
            "    probability_score: 0.151027903\n",
            "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
            "    severity_score: 0.120853223\n",
            "  }\n",
            "}\n",
            "model_version: \"gemini-1.5-flash-002\"\n",
            "usage_metadata {\n",
            "  prompt_token_count: 260\n",
            "  candidates_token_count: 72\n",
            "  total_token_count: 332\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## debug with single item\n",
        "evaluation_df = run_evaluation(generation_model, [corpus_batch[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aO8JhIg1pYkE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [00:33<00:29,  2.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 43/100 [00:33<00:18,  3.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 44/100 [00:33<00:16,  3.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 47/100 [00:34<00:11,  4.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 49/100 [00:34<00:10,  5.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 51/100 [00:35<00:08,  5.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 53/100 [00:35<00:08,  5.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 54/100 [00:35<00:07,  5.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 57/100 [00:36<00:07,  5.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 59/100 [00:36<00:07,  5.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [00:36<00:06,  5.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 63/100 [00:37<00:06,  5.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 65/100 [00:37<00:05,  6.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 67/100 [00:37<00:05,  6.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 69/100 [00:38<00:04,  6.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 71/100 [00:38<00:04,  5.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 73/100 [00:38<00:04,  5.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 75/100 [00:39<00:04,  5.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 77/100 [00:39<00:03,  5.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 79/100 [00:39<00:03,  6.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 81/100 [00:40<00:03,  6.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 83/100 [00:40<00:02,  5.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 85/100 [00:40<00:02,  6.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 87/100 [00:41<00:02,  6.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 89/100 [00:41<00:01,  5.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 91/100 [00:41<00:01,  5.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 93/100 [00:42<00:01,  6.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 95/100 [00:42<00:00,  6.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 97/100 [00:42<00:00,  6.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 99/100 [00:43<00:00,  5.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:43<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        }
      ],
      "source": [
        "# Run evaluation using loaded model and test data corpus\n",
        "evaluation_df = run_evaluation(generation_model, corpus_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "H4VUFeb9tRBP"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>summary</th>\n",
              "      <th>generated_summary</th>\n",
              "      <th>scores</th>\n",
              "      <th>rouge1_precision</th>\n",
              "      <th>rouge1_recall</th>\n",
              "      <th>rouge1_fmeasure</th>\n",
              "      <th>rouge2_precision</th>\n",
              "      <th>rouge2_recall</th>\n",
              "      <th>rouge2_fmeasure</th>\n",
              "      <th>rougeL_precision</th>\n",
              "      <th>rougeL_recall</th>\n",
              "      <th>rougeL_fmeasure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hold your arm out flat in front of you with yo...</td>\n",
              "      <td>Squeeze a line of lotion onto the tops of both...</td>\n",
              "      <td>This describes a method for applying lotion to...</td>\n",
              "      <td>{'rouge1': (0.22807017543859648, 0.41935483870...</td>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document  \\\n",
              "0  Hold your arm out flat in front of you with yo...   \n",
              "\n",
              "                                             summary  \\\n",
              "0  Squeeze a line of lotion onto the tops of both...   \n",
              "\n",
              "                                   generated_summary  \\\n",
              "0  This describes a method for applying lotion to...   \n",
              "\n",
              "                                              scores  rouge1_precision  \\\n",
              "0  {'rouge1': (0.22807017543859648, 0.41935483870...           0.22807   \n",
              "\n",
              "   rouge1_recall  rouge1_fmeasure  rouge2_precision  rouge2_recall  \\\n",
              "0       0.419355         0.295455             0.125       0.233333   \n",
              "\n",
              "   rouge2_fmeasure  rougeL_precision  rougeL_recall  rougeL_fmeasure  \n",
              "0         0.162791          0.192982       0.354839             0.25  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "18m75w6m0R10"
      },
      "outputs": [],
      "source": [
        "evaluation_df_stats = evaluation_df.dropna().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dYokPWqdUIMv"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1_precision</th>\n",
              "      <th>rouge1_recall</th>\n",
              "      <th>rouge1_fmeasure</th>\n",
              "      <th>rouge2_precision</th>\n",
              "      <th>rouge2_recall</th>\n",
              "      <th>rouge2_fmeasure</th>\n",
              "      <th>rougeL_precision</th>\n",
              "      <th>rougeL_recall</th>\n",
              "      <th>rougeL_fmeasure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.22807</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>0.295455</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.162791</td>\n",
              "      <td>0.192982</td>\n",
              "      <td>0.354839</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       rouge1_precision  rouge1_recall  rouge1_fmeasure  rouge2_precision  \\\n",
              "count           1.00000       1.000000         1.000000             1.000   \n",
              "mean            0.22807       0.419355         0.295455             0.125   \n",
              "std                 NaN            NaN              NaN               NaN   \n",
              "min             0.22807       0.419355         0.295455             0.125   \n",
              "25%             0.22807       0.419355         0.295455             0.125   \n",
              "50%             0.22807       0.419355         0.295455             0.125   \n",
              "75%             0.22807       0.419355         0.295455             0.125   \n",
              "max             0.22807       0.419355         0.295455             0.125   \n",
              "\n",
              "       rouge2_recall  rouge2_fmeasure  rougeL_precision  rougeL_recall  \\\n",
              "count       1.000000         1.000000          1.000000       1.000000   \n",
              "mean        0.233333         0.162791          0.192982       0.354839   \n",
              "std              NaN              NaN               NaN            NaN   \n",
              "min         0.233333         0.162791          0.192982       0.354839   \n",
              "25%         0.233333         0.162791          0.192982       0.354839   \n",
              "50%         0.233333         0.162791          0.192982       0.354839   \n",
              "75%         0.233333         0.162791          0.192982       0.354839   \n",
              "max         0.233333         0.162791          0.192982       0.354839   \n",
              "\n",
              "       rougeL_fmeasure  \n",
              "count             1.00  \n",
              "mean              0.25  \n",
              "std                NaN  \n",
              "min               0.25  \n",
              "25%               0.25  \n",
              "50%               0.25  \n",
              "75%               0.25  \n",
              "max               0.25  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Statistics of the evaluation dataframe.\n",
        "evaluation_df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4fdVjY_JWcmq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean rougeL_precision is 0.19298245614035087\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean rougeL_precision is\", evaluation_df_stats.rougeL_precision[\"mean\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMb3E0YEqL2"
      },
      "source": [
        "## Step7: Fine-tune the Model\n",
        "\n",
        " - `source_model`: Specifies the base Gemini model version you want to fine-tune.\n",
        " - `train_dataset`: Path to your training data in JSONL format.\n",
        "\n",
        "  *Optional parameters*\n",
        " - `validation_dataset`: If provided, this data is used to evaluate the model during tuning.\n",
        " - `tuned_model_display_name`: Display name for the tuned model.\n",
        " - `epochs`: The number of training epochs to run.\n",
        " - `learning_rate_multiplier`: A value to scale the learning rate during training.\n",
        " - `adapter_size` : Gemini 1.5 Pro supports Adapter length [1, 4], default value is 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e81137766c6"
      },
      "source": [
        "**Note: The default hyperparameter settings are optimized for optimal performance based on rigorous testing and are recommended for initial use. Users may customize these parameters to address specific performance requirements.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vQM2vDBZ27b_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.tuning._tuning:Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SupervisedTuningJob created. Resource name: projects/1082216977346/locations/us-central1/tuningJobs/3682657757346922496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.tuning._tuning:SupervisedTuningJob created. Resource name: projects/1082216977346/locations/us-central1/tuningJobs/3682657757346922496\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To use this SupervisedTuningJob in another session:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.tuning._tuning:To use this SupervisedTuningJob in another session:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tuning_job = sft.SupervisedTuningJob('projects/1082216977346/locations/us-central1/tuningJobs/3682657757346922496')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.tuning._tuning:tuning_job = sft.SupervisedTuningJob('projects/1082216977346/locations/us-central1/tuningJobs/3682657757346922496')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View Tuning Job:\n",
            "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/3682657757346922496?project=1082216977346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.tuning._tuning:View Tuning Job:\n",
            "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/3682657757346922496?project=1082216977346\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-fa6ae41f-f7f0-4a7f-8b82-1bf7ccefcced\" href=\"#view-view-vertex-resource-fa6ae41f-f7f0-4a7f-8b82-1bf7ccefcced\">\n",
              "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
              "          <span>View Tuning Job</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-fa6ae41f-f7f0-4a7f-8b82-1bf7ccefcced');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/3682657757346922496?project=1082216977346');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/3682657757346922496?project=1082216977346', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tuned_model_display_name = \"test_model_Wikilingua\"  # @param {type:\"string\"}\n",
        "\n",
        "# Tune a model using `train` method.\n",
        "sft_tuning_job = sft.train(\n",
        "    source_model=base_model,\n",
        "    train_dataset=f\"{BUCKET_URI}/train/sft_train_samples.jsonl\",\n",
        "    # Optional:\n",
        "    validation_dataset=f\"{BUCKET_URI}/val/sft_val_samples.jsonl\",\n",
        "    tuned_model_display_name=tuned_model_display_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "yLlAgVjCNqXg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'projects/1082216977346/locations/us-central1/tuningJobs/3682657757346922496',\n",
              " 'tunedModelDisplayName': 'test_model_Wikilingua',\n",
              " 'baseModel': 'gemini-1.5-flash-002',\n",
              " 'supervisedTuningSpec': {'trainingDatasetUri': 'gs://training_data_cryptic-skyline-411516/train/sft_train_samples.jsonl',\n",
              "  'validationDatasetUri': 'gs://training_data_cryptic-skyline-411516/val/sft_val_samples.jsonl',\n",
              "  'hyperParameters': {}},\n",
              " 'state': 'JOB_STATE_PENDING',\n",
              " 'createTime': '2025-01-10T22:27:26.308902Z',\n",
              " 'updateTime': '2025-01-10T22:27:26.308902Z'}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the tuning job info.\n",
        "sft_tuning_job.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KLDCWHIJ6sxo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'projects/1082216977346/locations/us-central1/tuningJobs/3682657757346922496'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the resource name of the tuning job\n",
        "sft_tuning_job_name = sft_tuning_job.resource_name\n",
        "sft_tuning_job_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22QZ035C8GJ3"
      },
      "source": [
        "**Note: Tuning time depends on several factors, such as training data size, number of epochs, learning rate multiplier, etc.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1KX-_WyKeu"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ It will take ~30 mins for the model tuning job to complete on the provided dataset and set configurations/hyperparameters. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "2ma3P6tZ6suI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-8991a229-8864-480c-a63f-859ee07973cc\" href=\"#view-view-vertex-resource-8991a229-8864-480c-a63f-859ee07973cc\">\n",
              "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
              "          <span>View Experiment</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-8991a229-8864-480c-a63f-859ee07973cc');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20250110143110555620/runs?project=cryptic-skyline-411516');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/tuning-experiment-20250110143110555620/runs?project=cryptic-skyline-411516', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 2.75 s\n",
            "Wall time: 15min 38s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Wait for job completion\n",
        "while not sft_tuning_job.refresh().has_ended:\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "O8MdqMvJHOA7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'projects/1082216977346/locations/us-central1/models/4945081033713254400@1'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tuned model name\n",
        "tuned_model_name = sft_tuning_job.tuned_model_name\n",
        "tuned_model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "e1O1xCBS6spi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'projects/1082216977346/locations/us-central1/endpoints/5693087590182289408'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tuned model endpoint name\n",
        "tuned_model_endpoint_name = sft_tuning_job.tuned_model_endpoint_name\n",
        "tuned_model_endpoint_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DzlWWKpbGcu"
      },
      "source": [
        "### Step7 [a]: Tuning and evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psRbCfzwWz_g"
      },
      "source": [
        "#### Model tuning metrics\n",
        "\n",
        "- `/train_total_loss`: Loss for the tuning dataset at a training step.\n",
        "- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n",
        "- `/train_num_predictions`: Number of predicted tokens at a training step\n",
        "\n",
        "#### Model evaluation metrics:\n",
        "\n",
        "- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n",
        "- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n",
        "- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\n",
        "\n",
        "The metrics visualizations are available after the model tuning job completes. If you don't specify a validation dataset when you create the tuning job, only the visualizations for the tuning metrics are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "kopo80uFbHCl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'projects/1082216977346/locations/us-central1/metadataStores/default/contexts/tuning-experiment-20250110143110555620'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get resource name from tuning job.\n",
        "experiment_name = sft_tuning_job.experiment.resource_name\n",
        "experiment_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5J1LP3nCbNlg"
      },
      "outputs": [],
      "source": [
        "# Locate Vertex AI Experiment and Vertex AI Experiment Run\n",
        "experiment = aiplatform.Experiment(experiment_name=experiment_name)\n",
        "filter_str = metadata_utils._make_filter_string(\n",
        "    schema_title=\"system.ExperimentRun\",\n",
        "    parent_contexts=[experiment.resource_name],\n",
        ")\n",
        "experiment_run = context.Context.list(filter_str)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "htBrcQY1bPyh"
      },
      "outputs": [],
      "source": [
        "# Read data from Tensorboard\n",
        "tensorboard_run_name = f\"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}\"\n",
        "tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)\n",
        "metrics = tensorboard_run.read_time_series_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "uRZ-UZXcbYj5"
      },
      "outputs": [],
      "source": [
        "def get_metrics(metric: str = \"/train_total_loss\"):\n",
        "    \"\"\"\n",
        "    Get metrics from Tensorboard.\n",
        "\n",
        "    Args:\n",
        "      metric: metric name, eg. /train_total_loss or /eval_total_loss.\n",
        "    Returns:\n",
        "      steps: list of steps.\n",
        "      steps_loss: list of loss values.\n",
        "    \"\"\"\n",
        "    loss_values = metrics[metric].values\n",
        "    steps_loss = []\n",
        "    steps = []\n",
        "    for loss in loss_values:\n",
        "        steps_loss.append(loss.scalar.value)\n",
        "        steps.append(loss.step)\n",
        "    return steps, steps_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ImR4doLZblaH"
      },
      "outputs": [],
      "source": [
        "# Get Train and Eval Loss\n",
        "train_loss = get_metrics(metric=\"/train_total_loss\")\n",
        "eval_loss = get_metrics(metric=\"/eval_total_loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuN-m1Ikbn15"
      },
      "source": [
        "### Step7 [b]: Plot the metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "1KWWkVR5jQkA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.plotly.v1+json": {
              "config": {
                "plotlyServerURL": "https://plot.ly"
              },
              "data": [
                {
                  "mode": "lines",
                  "name": "Train Loss",
                  "type": "scatter",
                  "x": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    12,
                    13,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19,
                    20,
                    21,
                    22,
                    23,
                    24,
                    25,
                    26,
                    27,
                    28,
                    29,
                    30,
                    31,
                    32,
                    33,
                    34,
                    35,
                    36,
                    37,
                    38,
                    39,
                    40
                  ],
                  "xaxis": "x",
                  "y": [
                    3.597036838531494,
                    3.895817518234253,
                    3.9414358139038086,
                    3.745265245437622,
                    3.7330234050750732,
                    3.698406934738159,
                    3.6293585300445557,
                    3.4516184329986572,
                    3.28959059715271,
                    3.1505680084228516,
                    2.843498706817627,
                    2.5370278358459473,
                    2.455734968185425,
                    2.403921604156494,
                    2.2860310077667236,
                    2.1411783695220947,
                    2.0617008209228516,
                    2.0835001468658447,
                    1.9977437257766724,
                    1.9475618600845337,
                    1.8612438440322876,
                    1.9219998121261597,
                    1.8700309991836548,
                    1.7811548709869385,
                    1.8735295534133911,
                    1.7050871849060059,
                    1.6683282852172852,
                    1.6812584400177002,
                    1.7041441202163696,
                    1.616875171661377,
                    1.6360622644424438,
                    1.5834757089614868,
                    1.5409289598464966,
                    1.5460457801818848,
                    1.6465373039245605,
                    1.5789324045181274,
                    1.5505321025848389,
                    1.5870591402053833,
                    1.50832200050354,
                    1.5013487339019775
                  ],
                  "yaxis": "y"
                },
                {
                  "mode": "lines",
                  "name": "Eval Loss",
                  "type": "scatter",
                  "x": [
                    1,
                    40
                  ],
                  "xaxis": "x2",
                  "y": [
                    4.221062660217285,
                    1.54749596118927
                  ],
                  "yaxis": "y2"
                }
              ],
              "layout": {
                "annotations": [
                  {
                    "font": {
                      "size": 16
                    },
                    "showarrow": false,
                    "text": "Train Loss",
                    "x": 0.225,
                    "xanchor": "center",
                    "xref": "paper",
                    "y": 1,
                    "yanchor": "bottom",
                    "yref": "paper"
                  },
                  {
                    "font": {
                      "size": 16
                    },
                    "showarrow": false,
                    "text": "Eval Loss",
                    "x": 0.775,
                    "xanchor": "center",
                    "xref": "paper",
                    "y": 1,
                    "yanchor": "bottom",
                    "yref": "paper"
                  }
                ],
                "template": {
                  "data": {
                    "bar": [
                      {
                        "error_x": {
                          "color": "#2a3f5f"
                        },
                        "error_y": {
                          "color": "#2a3f5f"
                        },
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "bar"
                      }
                    ],
                    "barpolar": [
                      {
                        "marker": {
                          "line": {
                            "color": "#E5ECF6",
                            "width": 0.5
                          },
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "barpolar"
                      }
                    ],
                    "carpet": [
                      {
                        "aaxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "baxis": {
                          "endlinecolor": "#2a3f5f",
                          "gridcolor": "white",
                          "linecolor": "white",
                          "minorgridcolor": "white",
                          "startlinecolor": "#2a3f5f"
                        },
                        "type": "carpet"
                      }
                    ],
                    "choropleth": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "choropleth"
                      }
                    ],
                    "contour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "contour"
                      }
                    ],
                    "contourcarpet": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "contourcarpet"
                      }
                    ],
                    "heatmap": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmap"
                      }
                    ],
                    "heatmapgl": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "heatmapgl"
                      }
                    ],
                    "histogram": [
                      {
                        "marker": {
                          "pattern": {
                            "fillmode": "overlay",
                            "size": 10,
                            "solidity": 0.2
                          }
                        },
                        "type": "histogram"
                      }
                    ],
                    "histogram2d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2d"
                      }
                    ],
                    "histogram2dcontour": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "histogram2dcontour"
                      }
                    ],
                    "mesh3d": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "type": "mesh3d"
                      }
                    ],
                    "parcoords": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "parcoords"
                      }
                    ],
                    "pie": [
                      {
                        "automargin": true,
                        "type": "pie"
                      }
                    ],
                    "scatter": [
                      {
                        "fillpattern": {
                          "fillmode": "overlay",
                          "size": 10,
                          "solidity": 0.2
                        },
                        "type": "scatter"
                      }
                    ],
                    "scatter3d": [
                      {
                        "line": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatter3d"
                      }
                    ],
                    "scattercarpet": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattercarpet"
                      }
                    ],
                    "scattergeo": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergeo"
                      }
                    ],
                    "scattergl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattergl"
                      }
                    ],
                    "scattermapbox": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scattermapbox"
                      }
                    ],
                    "scatterpolar": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolar"
                      }
                    ],
                    "scatterpolargl": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterpolargl"
                      }
                    ],
                    "scatterternary": [
                      {
                        "marker": {
                          "colorbar": {
                            "outlinewidth": 0,
                            "ticks": ""
                          }
                        },
                        "type": "scatterternary"
                      }
                    ],
                    "surface": [
                      {
                        "colorbar": {
                          "outlinewidth": 0,
                          "ticks": ""
                        },
                        "colorscale": [
                          [
                            0,
                            "#0d0887"
                          ],
                          [
                            0.1111111111111111,
                            "#46039f"
                          ],
                          [
                            0.2222222222222222,
                            "#7201a8"
                          ],
                          [
                            0.3333333333333333,
                            "#9c179e"
                          ],
                          [
                            0.4444444444444444,
                            "#bd3786"
                          ],
                          [
                            0.5555555555555556,
                            "#d8576b"
                          ],
                          [
                            0.6666666666666666,
                            "#ed7953"
                          ],
                          [
                            0.7777777777777778,
                            "#fb9f3a"
                          ],
                          [
                            0.8888888888888888,
                            "#fdca26"
                          ],
                          [
                            1,
                            "#f0f921"
                          ]
                        ],
                        "type": "surface"
                      }
                    ],
                    "table": [
                      {
                        "cells": {
                          "fill": {
                            "color": "#EBF0F8"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "header": {
                          "fill": {
                            "color": "#C8D4E3"
                          },
                          "line": {
                            "color": "white"
                          }
                        },
                        "type": "table"
                      }
                    ]
                  },
                  "layout": {
                    "annotationdefaults": {
                      "arrowcolor": "#2a3f5f",
                      "arrowhead": 0,
                      "arrowwidth": 1
                    },
                    "autotypenumbers": "strict",
                    "coloraxis": {
                      "colorbar": {
                        "outlinewidth": 0,
                        "ticks": ""
                      }
                    },
                    "colorscale": {
                      "diverging": [
                        [
                          0,
                          "#8e0152"
                        ],
                        [
                          0.1,
                          "#c51b7d"
                        ],
                        [
                          0.2,
                          "#de77ae"
                        ],
                        [
                          0.3,
                          "#f1b6da"
                        ],
                        [
                          0.4,
                          "#fde0ef"
                        ],
                        [
                          0.5,
                          "#f7f7f7"
                        ],
                        [
                          0.6,
                          "#e6f5d0"
                        ],
                        [
                          0.7,
                          "#b8e186"
                        ],
                        [
                          0.8,
                          "#7fbc41"
                        ],
                        [
                          0.9,
                          "#4d9221"
                        ],
                        [
                          1,
                          "#276419"
                        ]
                      ],
                      "sequential": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ],
                      "sequentialminus": [
                        [
                          0,
                          "#0d0887"
                        ],
                        [
                          0.1111111111111111,
                          "#46039f"
                        ],
                        [
                          0.2222222222222222,
                          "#7201a8"
                        ],
                        [
                          0.3333333333333333,
                          "#9c179e"
                        ],
                        [
                          0.4444444444444444,
                          "#bd3786"
                        ],
                        [
                          0.5555555555555556,
                          "#d8576b"
                        ],
                        [
                          0.6666666666666666,
                          "#ed7953"
                        ],
                        [
                          0.7777777777777778,
                          "#fb9f3a"
                        ],
                        [
                          0.8888888888888888,
                          "#fdca26"
                        ],
                        [
                          1,
                          "#f0f921"
                        ]
                      ]
                    },
                    "colorway": [
                      "#636efa",
                      "#EF553B",
                      "#00cc96",
                      "#ab63fa",
                      "#FFA15A",
                      "#19d3f3",
                      "#FF6692",
                      "#B6E880",
                      "#FF97FF",
                      "#FECB52"
                    ],
                    "font": {
                      "color": "#2a3f5f"
                    },
                    "geo": {
                      "bgcolor": "white",
                      "lakecolor": "white",
                      "landcolor": "#E5ECF6",
                      "showlakes": true,
                      "showland": true,
                      "subunitcolor": "white"
                    },
                    "hoverlabel": {
                      "align": "left"
                    },
                    "hovermode": "closest",
                    "mapbox": {
                      "style": "light"
                    },
                    "paper_bgcolor": "white",
                    "plot_bgcolor": "#E5ECF6",
                    "polar": {
                      "angularaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "radialaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "scene": {
                      "xaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "yaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      },
                      "zaxis": {
                        "backgroundcolor": "#E5ECF6",
                        "gridcolor": "white",
                        "gridwidth": 2,
                        "linecolor": "white",
                        "showbackground": true,
                        "ticks": "",
                        "zerolinecolor": "white"
                      }
                    },
                    "shapedefaults": {
                      "line": {
                        "color": "#2a3f5f"
                      }
                    },
                    "ternary": {
                      "aaxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "baxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      },
                      "bgcolor": "#E5ECF6",
                      "caxis": {
                        "gridcolor": "white",
                        "linecolor": "white",
                        "ticks": ""
                      }
                    },
                    "title": {
                      "x": 0.05
                    },
                    "xaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    },
                    "yaxis": {
                      "automargin": true,
                      "gridcolor": "white",
                      "linecolor": "white",
                      "ticks": "",
                      "title": {
                        "standoff": 15
                      },
                      "zerolinecolor": "white",
                      "zerolinewidth": 2
                    }
                  }
                },
                "title": {
                  "text": "Train and Eval Loss"
                },
                "xaxis": {
                  "anchor": "y",
                  "domain": [
                    0,
                    0.45
                  ],
                  "title": {
                    "text": "Steps"
                  }
                },
                "xaxis2": {
                  "anchor": "y2",
                  "domain": [
                    0.55,
                    1
                  ],
                  "title": {
                    "text": "Steps"
                  }
                },
                "yaxis": {
                  "anchor": "x",
                  "domain": [
                    0,
                    1
                  ],
                  "title": {
                    "text": "Loss"
                  }
                },
                "yaxis2": {
                  "anchor": "x2",
                  "domain": [
                    0,
                    1
                  ],
                  "title": {
                    "text": "Loss"
                  }
                }
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the train and eval loss metrics using Plotly python library\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2, shared_xaxes=True, subplot_titles=(\"Train Loss\", \"Eval Loss\")\n",
        ")\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=train_loss[0], y=train_loss[1], name=\"Train Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=1,\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=eval_loss[0], y=eval_loss[1], name=\"Eval Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=2,\n",
        ")\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(title=\"Train and Eval Loss\", xaxis_title=\"Steps\", yaxis_title=\"Loss\")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Steps\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"Loss\")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY-eiVk0FI-M"
      },
      "source": [
        "## Step8: Load the Tuned Model\n",
        "\n",
        " - Load the fine-tuned model using `GenerativeModel` class with the tuning job model endpoint name.\n",
        "\n",
        " - Test the tuned model with the following prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "GiJ831VMDQNy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nHold your arm out flat in front of you with your elbow bent. The top of your forearm should form a level surface. Apply a line of lotion from the back of your hand up your arm almost to the crease of your elbow. Squeeze lotion onto both forearms.  Do not rub the lotion into your arms, rather let it sit on your arm in the line you squeezed. You can use as much or as little lotion as you feel is necessary to cover your back completely. Bend your elbows and reach both of your arms behind you, placing the lotion covered forearms against your back. Depending on how flexible you are, this may hurt a little. It might be easier to place one arm behind your back at a time. If you have shoulder pain or are not very flexible, this method may not work well for you. Rub your forearms and the backs of your hands up and down your back like windshield wipers covering as much of your back as you can. You can use your left arm first to cover your left side and then place your right arm behind and use it to cover the right side of your back. Repeat this process as necessary if you don’t feel like you got enough lotion on your back.\\n\\nProvide a summary of the article in two or three sentences:\\n\\n\\n'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "65SYYpaNT4QR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***Testing***\n",
            "candidates {\n",
            "  content {\n",
            "    role: \"model\"\n",
            "    parts {\n",
            "      text: \"Hold your arm out in front of you with your elbow bent. Squeeze a line of lotion onto your forearm. Reach your arm behind your back. Rub your forearm up and down your back. Repeat with your other arm.\\n\\n\"\n",
            "    }\n",
            "  }\n",
            "  avg_logprobs: -0.33572750091552733\n",
            "  finish_reason: STOP\n",
            "}\n",
            "usage_metadata {\n",
            "  prompt_token_count: 261\n",
            "  candidates_token_count: 45\n",
            "  total_token_count: 306\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    tuned_genai_model = GenerativeModel(tuned_model_endpoint_name)\n",
        "    # Test with the loaded model.\n",
        "    print(\"***Testing***\")\n",
        "    print(\n",
        "        tuned_genai_model.generate_content(\n",
        "            contents=prompt, generation_config=generation_config\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    print(\"State:\", sft_tuning_job.state)\n",
        "    print(\"Error:\", sft_tuning_job.error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7acd61d12d"
      },
      "source": [
        "```\n",
        "candidates {\n",
        "  content {\n",
        "    role: \"model\"\n",
        "    parts {\n",
        "      text: \"Squeeze a line of lotion onto the top of each forearm. Place your forearms behind your back. Rub your forearms up and down your back.\\n\\n\"\n",
        "    }\n",
        "  }\n",
        "  finish_reason: STOP\n",
        "  avg_logprobs: -0.39081838726997375\n",
        "}\n",
        "usage_metadata {\n",
        "  prompt_token_count: 261\n",
        "  candidates_token_count: 32\n",
        "  total_token_count: 293\n",
        "}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d54ce2b88af3"
      },
      "source": [
        "- We can clearly see the difference between summary generated pre and post tuning, as tuned summary is more inline with the ground truth format (**Note**: Pre and Post outputs, might vary based on the set parameters.)\n",
        "\n",
        "  - *Pre*: `This article describes a method for applying lotion to your own back using your forearms. The technique involves squeezing lotion in a line along your forearms, bending your elbows, and rubbing your arms against your back in a windshield wiper motion. This method may not be suitable for individuals with shoulder pain or limited flexibility.`\n",
        "  - *Post*: `Squeeze a line of lotion onto the top of each forearm. Place your forearms behind your back. Rub your forearms up and down your back`\n",
        "  - *Ground Truth*:` Squeeze a line of lotion onto the tops of both forearms and the backs of your hands. Place your arms behind your back. Move your arms in a windshield wiper motion.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYsIpFakU4CC"
      },
      "source": [
        "## Step9: Evaluation post model tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwlCcKPZ62Of"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ It will take ~5 mins for the evaluation on the provided batch. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_text': 'Hold your arm out flat in front of you with your elbow bent. The top of your forearm should form a level surface. Apply a line of lotion from the back of your hand up your arm almost to the crease of your elbow. Squeeze lotion onto both forearms.  Do not rub the lotion into your arms, rather let it sit on your arm in the line you squeezed. You can use as much or as little lotion as you feel is necessary to cover your back completely. Bend your elbows and reach both of your arms behind you, placing the lotion covered forearms against your back. Depending on how flexible you are, this may hurt a little. It might be easier to place one arm behind your back at a time. If you have shoulder pain or are not very flexible, this method may not work well for you. Rub your forearms and the backs of your hands up and down your back like windshield wipers covering as much of your back as you can. You can use your left arm first to cover your left side and then place your right arm behind and use it to cover the right side of your back. Repeat this process as necessary if you don’t feel like you got enough lotion on your back.\\n\\nProvide a summary of the article in two or three sentences:\\n\\n',\n",
              " 'output_text': 'Squeeze a line of lotion onto the tops of both forearms and the backs of your hands. Place your arms behind your back. Move your arms in a windshield wiper motion.'}"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# debug run evaluation\n",
        "evaluation_df_post_tuning = run_evaluation(tuned_genai_model, [corpus_batch[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KrBk1amTU3r2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 47/100 [01:08<00:43,  1.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 49/100 [01:09<00:25,  2.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 51/100 [01:09<00:16,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 53/100 [01:09<00:11,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 55/100 [01:10<00:09,  4.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 57/100 [01:10<00:07,  5.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 59/100 [01:10<00:07,  5.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [01:11<00:06,  5.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 62/100 [01:11<00:06,  6.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 65/100 [01:11<00:05,  5.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 67/100 [01:12<00:05,  6.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 69/100 [01:12<00:05,  5.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 71/100 [01:12<00:05,  5.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 73/100 [01:13<00:04,  5.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 75/100 [01:13<00:04,  5.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 77/100 [01:13<00:03,  6.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 79/100 [01:14<00:03,  6.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 80/100 [01:14<00:03,  6.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 86/100 [01:20<00:07,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n",
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 93/100 [01:29<00:06,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 95/100 [01:30<00:04,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 429 Online prediction request quota exceeded for gemini-1.5-flash. Please try again later with backoff.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:38<00:00,  1.02it/s]\n"
          ]
        }
      ],
      "source": [
        "# run evaluation\n",
        "evaluation_df_post_tuning = run_evaluation(tuned_genai_model, corpus_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "ONnlEkSex-iO"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>summary</th>\n",
              "      <th>generated_summary</th>\n",
              "      <th>scores</th>\n",
              "      <th>rouge1_precision</th>\n",
              "      <th>rouge1_recall</th>\n",
              "      <th>rouge1_fmeasure</th>\n",
              "      <th>rouge2_precision</th>\n",
              "      <th>rouge2_recall</th>\n",
              "      <th>rouge2_fmeasure</th>\n",
              "      <th>rougeL_precision</th>\n",
              "      <th>rougeL_recall</th>\n",
              "      <th>rougeL_fmeasure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hold your arm out flat in front of you with yo...</td>\n",
              "      <td>Squeeze a line of lotion onto the tops of both...</td>\n",
              "      <td>Hold your arm out in front of you with your el...</td>\n",
              "      <td>{'rouge1': (0.5135135135135135, 0.612903225806...</td>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            document  \\\n",
              "0  Hold your arm out flat in front of you with yo...   \n",
              "\n",
              "                                             summary  \\\n",
              "0  Squeeze a line of lotion onto the tops of both...   \n",
              "\n",
              "                                   generated_summary  \\\n",
              "0  Hold your arm out in front of you with your el...   \n",
              "\n",
              "                                              scores  rouge1_precision  \\\n",
              "0  {'rouge1': (0.5135135135135135, 0.612903225806...          0.513514   \n",
              "\n",
              "   rouge1_recall  rouge1_fmeasure  rouge2_precision  rouge2_recall  \\\n",
              "0       0.612903         0.558824              0.25            0.3   \n",
              "\n",
              "   rouge2_fmeasure  rougeL_precision  rougeL_recall  rougeL_fmeasure  \n",
              "0         0.272727          0.378378       0.451613         0.411765  "
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluation_df_post_tuning.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "xDJrlD8O0B4d"
      },
      "outputs": [],
      "source": [
        "evaluation_df_post_tuning_stats = evaluation_df_post_tuning.dropna().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "c24-mE12y4Nm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1_precision</th>\n",
              "      <th>rouge1_recall</th>\n",
              "      <th>rouge1_fmeasure</th>\n",
              "      <th>rouge2_precision</th>\n",
              "      <th>rouge2_recall</th>\n",
              "      <th>rouge2_fmeasure</th>\n",
              "      <th>rougeL_precision</th>\n",
              "      <th>rougeL_recall</th>\n",
              "      <th>rougeL_fmeasure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.513514</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       rouge1_precision  rouge1_recall  rouge1_fmeasure  rouge2_precision  \\\n",
              "count          1.000000       1.000000         1.000000              1.00   \n",
              "mean           0.513514       0.612903         0.558824              0.25   \n",
              "std                 NaN            NaN              NaN               NaN   \n",
              "min            0.513514       0.612903         0.558824              0.25   \n",
              "25%            0.513514       0.612903         0.558824              0.25   \n",
              "50%            0.513514       0.612903         0.558824              0.25   \n",
              "75%            0.513514       0.612903         0.558824              0.25   \n",
              "max            0.513514       0.612903         0.558824              0.25   \n",
              "\n",
              "       rouge2_recall  rouge2_fmeasure  rougeL_precision  rougeL_recall  \\\n",
              "count            1.0         1.000000          1.000000       1.000000   \n",
              "mean             0.3         0.272727          0.378378       0.451613   \n",
              "std              NaN              NaN               NaN            NaN   \n",
              "min              0.3         0.272727          0.378378       0.451613   \n",
              "25%              0.3         0.272727          0.378378       0.451613   \n",
              "50%              0.3         0.272727          0.378378       0.451613   \n",
              "75%              0.3         0.272727          0.378378       0.451613   \n",
              "max              0.3         0.272727          0.378378       0.451613   \n",
              "\n",
              "       rougeL_fmeasure  \n",
              "count         1.000000  \n",
              "mean          0.411765  \n",
              "std                NaN  \n",
              "min           0.411765  \n",
              "25%           0.411765  \n",
              "50%           0.411765  \n",
              "75%           0.411765  \n",
              "max           0.411765  "
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Statistics of the evaluation dataframe post model tuning.\n",
        "evaluation_df_post_tuning_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "9VU-8Ql2bqlo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean rougeL_precision is 0.3783783783783784\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Mean rougeL_precision is\", evaluation_df_post_tuning_stats.rougeL_precision[\"mean\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q8hN7SE08-X"
      },
      "source": [
        "#### Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "j0ctGzdnznYO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3783783783783784\n",
            "0.19298245614035087\n",
            "Model tuning has improved the rougeL_precision by 96.07% (result might differ based on each tuning iteration)\n"
          ]
        }
      ],
      "source": [
        "improvement = round(\n",
        "    (\n",
        "        (\n",
        "            evaluation_df_post_tuning_stats.rougeL_precision[\"mean\"]\n",
        "            - evaluation_df_stats.rougeL_precision[\"mean\"]\n",
        "        )\n",
        "        / evaluation_df_stats.rougeL_precision[\"mean\"]\n",
        "    )\n",
        "    * 100,\n",
        "    2,\n",
        ")\n",
        "print(\n",
        "    f\"Model tuning has improved the rougeL_precision by {improvement}% (result might differ based on each tuning iteration)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQkpAMnpw-jH"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esra6YPgxBiV"
      },
      "source": [
        "Performance could be further improved:\n",
        "- By adding more training samples. In general, improve your training data quality and/or quantity towards getting a more diverse and comprehensive dataset for your task\n",
        "- By tuning the hyperparameters, such as epochs and learning rate multiplier\n",
        "  - To find the optimal number of epochs for your dataset, we recommend experimenting with different values. While increasing epochs can lead to better performance, it's important to be mindful of overfitting, especially with smaller datasets. If you see signs of overfitting, reducing the number of epochs can help mitigate the issue\n",
        "- You may try different prompt structures/formats and opt for the one with better performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e6fd3649040"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5528064b2cdf"
      },
      "source": [
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial.\n",
        "\n",
        "Refer to this [instructions](https://cloud.google.com/vertex-ai/docs/tutorials/image-classification-custom/cleanup#delete_resources) to delete the resources from console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4dd0f5d2a21"
      },
      "outputs": [],
      "source": [
        "# Delete Experiment.\n",
        "delete_experiments = True\n",
        "if delete_experiments:\n",
        "    experiments_list = aiplatform.Experiment.list()\n",
        "    for experiment in experiments_list:\n",
        "        if experiment.resource_name == experiment_name:\n",
        "            print(experiment.resource_name)\n",
        "            experiment.delete()\n",
        "            break\n",
        "\n",
        "print(\"***\" * 10)\n",
        "\n",
        "# Delete Endpoint.\n",
        "delete_endpoint = True\n",
        "# If force is set to True, all deployed models on this\n",
        "# Endpoint will be first undeployed.\n",
        "if delete_endpoint:\n",
        "    for endpoint in aiplatform.Endpoint.list():\n",
        "        if endpoint.resource_name == tuned_model_endpoint_name:\n",
        "            print(endpoint.resource_name)\n",
        "            endpoint.delete(force=True)\n",
        "            break\n",
        "\n",
        "print(\"***\" * 10)\n",
        "\n",
        "# Delete Cloud Storage Bucket.\n",
        "delete_bucket = True\n",
        "if delete_bucket:\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "supervised_finetuning_using_gemini.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
